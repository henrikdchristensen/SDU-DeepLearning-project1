\section{Regularization}




notes:
- Convolutional network with 4 convolutional layers. More conv layers, deep layers, neurons leads to overfitting?
- why the convolutional layers?
Since the input images are in color and 224 pixels on each axis, inputting that image directly into the fully connected layers would result in a 224*224*3 = 150528 input size into the model. We would like to reduce this further, so the fully connected layers does not get an input of 150528. The convolution layers extract features, which makes it possible for the neural network to recognise a specific feature no matter where it is in the image, instead of having to learn that feature for every possible position in an image. We have 4 layers of convolution. This is the result of trial and error.
- We considered using a residual NN (put in results instead?)
- why the loss function?
We use cross entropy loss as our loss function since this loss function is a popular and good loss function for classification tasks.
- why the optimizer?
We use adam as the optimizer since it combines the strength of several other optimizers. It is a generally good optimizer and since we are not using L2 regularization, we do not need to user adamW.- why the optimizer?
- why the number of epochs?
We chose to train the models in 100 epochs. This is a sufficiently high number that the model gets to be properly trained, but still a reasonable number to keep within a reasonable time to train the model. More epochs is good until a certain point when the model starts overfitting. As an improvement we could have included some early stopping in case the model began overfitting. - why the number of epochs? purely time constrain?
- why the learning rate?
We chose a learning rate of 0.001. A learning rate that is too small could result in a very slow learning process that might get stuck, and a too big learning rate could result in learning a suboptimal set of weights or an unstable learning process. A smaller learning rate requires more epochs for a better result. The default learning rate is 0.01. We chose a smaller learning rate since we wanted our learning process to be very stable, and since we run 100 epochs it seemed appropriate.


==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
ConvolutionalNetwork                     [64, 2]                   --
├─Sequential: 1-1                        [64, 512, 6, 6]           3,911,808
│    └─Conv2d: 2-1                       [64, 64, 200, 200]        1,792
│    └─BatchNorm2d: 2-2                  [64, 64, 200, 200]        128
├─Sequential: 1-10                       --                        (recursive)
│    └─ReLU: 2-3                         [64, 64, 200, 200]        --
├─Sequential: 1-11                       --                        (recursive)
│    └─MaxPool2d: 2-4                    [64, 64, 100, 100]        --
│    └─Conv2d: 2-5                       [64, 128, 100, 100]       73,856
│    └─BatchNorm2d: 2-6                  [64, 128, 100, 100]       256
├─Sequential: 1-10                       --                        (recursive)
│    └─ReLU: 2-7                         [64, 128, 100, 100]       --
├─Sequential: 1-11                       --                        (recursive)
│    └─MaxPool2d: 2-8                    [64, 128, 50, 50]         --
│    └─Conv2d: 2-9                       [64, 256, 50, 50]         295,168
│    └─BatchNorm2d: 2-10                 [64, 256, 50, 50]         512
├─Sequential: 1-10                       --                        (recursive)
│    └─ReLU: 2-11                        [64, 256, 50, 50]         --
├─Sequential: 1-11                       --                        (recursive)
│    └─MaxPool2d: 2-12                   [64, 256, 25, 25]         --
│    └─Conv2d: 2-13                      [64, 512, 25, 25]         1,180,160
│    └─BatchNorm2d: 2-14                 [64, 512, 25, 25]         1,024
├─Sequential: 1-10                       --                        (recursive)
│    └─ReLU: 2-15                        [64, 512, 25, 25]         --
├─Sequential: 1-11                       --                        (recursive)
│    └─MaxPool2d: 2-16                   [64, 512, 12, 12]         --
│    └─Conv2d: 2-17                      [64, 512, 12, 12]         2,359,808
│    └─BatchNorm2d: 2-18                 [64, 512, 12, 12]         1,024
├─Sequential: 1-10                       --                        (recursive)
│    └─ReLU: 2-19                        [64, 512, 12, 12]         --
├─Sequential: 1-11                       --                        (recursive)
│    └─MaxPool2d: 2-20                   [64, 512, 6, 6]           --
├─Sequential: 1-12                       [64, 2]                   --
│    └─Linear: 2-21                      [64, 1024]                18,875,392
│    └─ReLU: 2-22                        [64, 1024]                --
│    └─Dropout: 2-23                     [64, 1024]                --
│    └─Linear: 2-24                      [64, 256]                 262,400
│    └─ReLU: 2-25                        [64, 256]                 --
│    └─Dropout: 2-26                     [64, 256]                 --
│    └─Linear: 2-27                      [64, 2]                   514
==========================================================================================
Total params: 26,963,842
Trainable params: 26,963,842
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 169.26
==========================================================================================
Input size (MB): 30.72
Forward/backward pass size (MB): 4991.35
Params size (MB): 92.21
Estimated Total Size (MB): 5114.28
==========================================================================================
Training and Validation Metrics:
Train Losses: [0.7538285665214062, 0.7224470470100641, 0.7145087048411369, 0.7013439275324345, 0.6994247082620859, 0.6918729450553656, 0.6945706307888031, 0.6944414246827364, 0.692684693261981, 0.6884838454425335, 0.6911793742328882, 0.6835229638963938, 0.6816752143204212, 0.6760735437273979, 0.686188843101263, 0.6775427553802729, 0.6771683655679226, 0.6777054630219936, 0.6794191356748343, 0.6812760829925537, 0.6827313583344221, 0.6713614780455828, 0.6808867491781712, 0.6748827509582043, 0.6701625287532806, 0.6704702731221914, 0.6690580099821091, 0.661839185282588, 0.6665689740329981, 0.6665738485753536, 0.6618479434400797, 0.6612277571111917, 0.6749424953013659, 0.6654958389699459, 0.6639538817107677, 0.6614651195704937, 0.663782462477684, 0.6556782815605402, 0.6611711159348488, 0.655089383944869, 0.655412757769227, 0.6460628230124712, 0.6547303386032581, 0.6624046843498945, 0.6412318591028452, 0.6599008701741695, 0.6517211142927408, 0.6458467468619347, 0.6486418452113867, 0.6452178824692965, 0.6385481748729944, 0.6612051595002413, 0.6586725898087025, 0.6441635675728321, 0.6484996024519205, 0.636792678385973, 0.6444840542972088, 0.6421012431383133, 0.6310699209570885, 0.6513198763132095, 0.6422351598739624, 0.628137294203043, 0.6259275823831558, 0.6233736053109169, 0.6308506745845079, 0.6273368783295155, 0.6403384599834681, 0.6327075473964214, 0.6318606082350016, 0.6227960297837853, 0.6208273805677891, 0.6281239315867424, 0.6337704490870237, 0.6133852042257786, 0.6348875090479851, 0.6248429920524359, 0.619644308462739, 0.6305358679965138, 0.6210433784872293, 0.6309873089194298]
Train Accuracies: [51.15, 50.35, 50.45, 52.95, 53.0, 53.3, 53.6, 52.9, 54.35, 54.15, 54.05, 56.6, 57.0, 59.15, 56.0, 57.55, 57.85, 57.15, 58.2, 57.15, 56.85, 59.25, 57.45, 58.9, 58.45, 58.1, 59.75, 60.8, 60.25, 59.9, 61.45, 61.3, 59.25, 59.8, 60.3, 60.15, 61.2, 62.85, 61.15, 62.4, 62.0, 62.3, 62.55, 61.4, 63.45, 61.5, 61.5, 63.95, 61.9, 62.55, 63.85, 60.85, 61.65, 62.55, 63.0, 63.5, 63.45, 63.9, 65.15, 62.15, 61.95, 64.4, 65.3, 66.7, 63.85, 65.15, 63.4, 64.7, 64.7, 63.9, 66.7, 63.65, 65.25, 66.8, 63.55, 65.5, 66.85, 64.5, 65.35, 64.5]
Val Losses: [0.6803641796112061, 0.6793376266956329, 0.6765377163887024, 0.6569146513938904, 0.6706378161907196, 0.6634692788124085, 0.6694955825805664, 0.6664822995662689, 0.6729614317417145, 0.6594395458698272, 0.6696122527122498, 0.6579982519149781, 0.6549152374267578, 0.6702766597270966, 0.6579103946685791, 0.6486445903778076, 0.6562822610139847, 0.6576030135154725, 0.6532828330993652, 0.6626104176044464, 0.658942985534668, 0.6761103957891464, 0.6618440806865692, 0.6481980919837952, 0.6459380030632019, 0.6521754622459411, 0.6472256422042847, 0.6531039476394653, 0.6481284976005555, 0.6370577096939087, 0.6434629738330842, 0.6323737978935242, 0.6437190413475037, 0.6415578454732895, 0.6446258127689362, 0.6257625490427017, 0.6260398983955383, 0.6229507923126221, 0.6241016745567322, 0.6357400238513946, 0.6231657713651657, 0.6299236744642258, 0.6035449951887131, 0.6314720749855042, 0.625438266992569, 0.603306433558464, 0.6131084114313126, 0.614875590801239, 0.6461936891078949, 0.6197546601295472, 0.6073786973953247, 0.6293708026409149, 0.6386557221412659, 0.6153988361358642, 0.6184587359428406, 0.6082427561283111, 0.6329834461212158, 0.5975041151046753, 0.6560354828834534, 0.609629625082016, 0.6278603345155715, 0.6116795361042022, 0.5807162761688233, 0.5914504826068878, 0.6047806918621064, 0.62668017745018, 0.5862917810678482, 0.5847371011972428, 0.5812803328037262, 0.6041006863117218, 0.6181731939315795, 0.607328724861145, 0.6880025714635849, 0.6009145021438599, 0.6008051991462707, 0.6344600141048431, 0.5814920514822006, 0.594786936044693, 0.601848304271698, 0.6055842846632004]
Val Accuracies: [65.0, 60.333333333333336, 59.5, 59.5, 64.0, 61.666666666666664, 63.833333333333336, 59.833333333333336, 57.833333333333336, 64.33333333333333, 63.333333333333336, 60.666666666666664, 66.5, 62.833333333333336, 65.0, 63.666666666666664, 59.5, 58.833333333333336, 65.16666666666667, 64.5, 61.5, 52.666666666666664, 62.666666666666664, 63.666666666666664, 61.666666666666664, 56.833333333333336, 65.33333333333333, 56.0, 66.0, 65.83333333333333, 64.0, 66.5, 67.5, 60.0, 57.166666666666664, 64.83333333333333, 68.66666666666667, 63.333333333333336, 68.0, 61.166666666666664, 67.0, 64.66666666666667, 69.5, 69.83333333333333, 63.0, 68.0, 63.833333333333336, 71.16666666666667, 65.16666666666667, 69.83333333333333, 66.83333333333333, 64.0, 67.83333333333333, 68.0, 63.166666666666664, 65.0, 59.333333333333336, 68.66666666666667, 66.33333333333333, 70.16666666666667, 64.66666666666667, 65.0, 71.33333333333333, 69.66666666666667, 70.66666666666667, 67.33333333333333, 72.0, 70.66666666666667, 68.5, 66.83333333333333, 69.0, 71.5, 54.5, 65.33333333333333, 71.66666666666667, 66.66666666666667, 70.0, 72.16666666666667, 63.5, 64.16666666666667]
